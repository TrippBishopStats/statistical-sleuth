---
title: "Chapter 2" 
author: "Tripp Bishop" 
format: html
editor: source
---
```{r setup}
#| echo: false
#| message: false

rm(list = ls())
library(tidyverse)
library(tidymodels)
```
We first create a dataframe to store our data.
```{r sparrow data}
survived <- rep(c(TRUE, FALSE), times=c(35,24))
humerus_length_in <- c(
  #survivors
  0.687,0.703,0.709,0.715,0.721,0.723,0.723,0.726,0.728,0.728,0.728,
  0.729,0.730,0.730,0.733,0.733,0.735,0.736,0.739,0.741,0.741,0.741,
  0.741,0.743,0.749,0.751,0.752,0.752,0.755,0.756,0.766,0.767,0.769,
  0.770,0.780,
  # victims
  0.659,0.689,0.702,0.703,0.709,0.713,0.720,0.720,0.726,0.726,0.729,
  0.731,0.736,0.737,0.738,0.738,0.739,0.743,0.744,0.745,0.752,0.752,
  0.754,0.765
)
df_sparrows <- tibble(humerus_length_in, survived) |>
  mutate(
    survived = factor(survived, labels = c("Perished", "Survived"))
  )
```
Since we have already collected and stored the data, the next step is to
calculate the test statistic. Here, we're interested in the difference in means
of the length of the humerus bone in the two groups of sparrows.
```{r create test statistic}
obs_diff <- df_sparrows |>
  specify(humerus_length_in ~ survived) |>
  calculate(stat = "diff in means", order = c("Survived", "Perished")) |>
  pull()
```
Now, we create a `null world` where we create a sampling distribution where the
survival of the individual birds is independent of their humerus length.
```{r creat null distribution}
sparrows_null <- df_sparrows |>
  specify(humerus_length_in ~ survived) |>
  hypothesise(null = "independence") |>
  generate(reps = 1500, type = "permute") |>
  calculate(stat = "diff in means", order = c("Survived", "Perished"))
p_value <- round(
  get_p_value(sparrows_null, obs_stat = obs_diff, direction = "both"),
  4
)
visualise(sparrows_null) +
  shade_p_value(obs_diff, direction = "both") +
  annotate("text", label = paste("p-value = ", p_value, sep=""), x = 0.014, y=150)
```
```{r t-test manual}
sd_died <- df_sparrows |>
  filter(survived == "Perished") |>
  summarise(
    std_dev = sd(humerus_length_in)
  ) |>
  pull()
sd_lived <- df_sparrows |>
  filter(survived == "Survived") |>
  summarise(
    std_dev = sd(humerus_length_in)
  ) |>
  pull()
pooled_sd <- sqrt((sd_died^2*23 + sd_lived^2*34)/57)
SE <- pooled_sd*sqrt(1/24 + 1/35)
t_stat <- obs_diff/(SE)
df <- 24 + 35 -2
p_value <- pt(t_stat, df, lower.tail = FALSE)
two_sided <- p_value*2
qt(0.025, 57)
qt(0.975, 57)
```
## Conceptual Exercises

### Problem 1
> In drawing conclusions about the Bumpus data , why might it be important to 
know whether all the birds were measured before the ones htat perished actually 
died?

If the birds' bodies change after death, this could distort the group
differences. It might be possible that the death of the birds is what is causing
a difference in mean humerus length.

There is also the possibility of measurement bias since the researcher measuring
the birds will know which ones are dead and alive and if they have a particular 
expectation it might result in them subtly changing the way that they make their
measurements of the individual specimens.

### Problem 2
> For comparing two population means when the population distributions have the 
same standard deviation, the standard deviation is sometimes referred to as a
nuisance parameter. Explain why it might be considered a nuisance.

The standard deviation is not generally of that much interest, but it must be
estimated from the data, because it is not usually known, in order to 
investigate the mean.

### Problem 3
>True or false? If the sample size is large, then the shape of the histogram of
the sample will be approximately normal, even if the population distribution is
not normal.

This is false. The histogram of the sample will approximate the population 
distribution. The larger the sample, the better the approximation of this
distribution.

### Problem 4
> True or false? If a sample size is large, then the shapre of the sampling
distribution will be approximately normal, even if the population distribution
is not normal.

This is true. This is the result of the central limit theorem. In particular,
the standard error of the sampling distribution will get smaller and tails of
the distribution will shrink.

### Problem 5
> Explain the relative merits of 90% and 99% levels of confidence.

The level of confidence is directly related to the width of the interval. A 
lower level of confidence results in a narrower interval which might provide
better constraint on the true value of the parameter, but you risk being wrong
in your estimate of the parameter and therefore the outcome of your hypothesis
test. You risk rejecting the null hypothesis when you should not. This is known
as a Type II error.

On the other hand, a high confidence level means that you are likely to capture
the true value of the parameter, but you are more likely to fail to reject the 
null hypothesis even though it is false. This is known as a Type I error.

The choice of confidence level is often governed by the penalty paid for making
a  Type I or II error. If a false possible would be a bad outcome, then a higher
confidence level is warranted. Perhaps you want to be very certain that you have
detected something real, in which case you would want to set $\alpha$ quite
small. On the other hand, if you want to avoid false negatives, a narrower
confidence interval will likely result in rejecting the null hypothesis.

### Problem 6
> What is wrong with the hypothesis that $\bar{Y}_2 - \bar{Y}_1 = 0$?

The hypothesis is about the population means $\mu_1, \mu_2$ not the sample 
means.

Kinda pedantic... but whatever.

### Problem 7
> In a study of the effects of marijuana use during pregnancy, measurements on 
babies of mothers who used marijuana during pregnancy were compared to 
measurements on babies of mothers who did not. A 95% confidence interval for the
difference in mean head circumference (nonuse minus use) was 0.61 to 1.19cm. 
What can be said from this statement about a *p*-value for the hypothesis that 
the mean difference is zero?

Since the interval does not contain zero, we know that the *p*-value is going to
be small. Since the confidence interval is $100(1-\alpha) = 95%$, we know that 
our threshold is $p=0.05$. We know that $p < 0.05$ given that the interval does 
not contain the hypothesised parameter value. 

### Problem 8
> Suppose the following statement is made in a statistical summary: "A 
comparison of breathing capacities of individuals in househols with low nitrogen
dioxide levels and individuals in households with high nitrogen dioxide levels 
indicated that there is not difference in the means 
(two-sided, *p*-value=0.24)". What is wrong with this statement?

The statement is too strong. When performing a hypothesis test, the null
hypothesis is not accepted, we simply fail to reject it. Stated another way, the
data collected are not consistent with the means being different, but that
doesn't rule out future data being incompatible with unequal means.

### Problem 9
> What is the difference (a) between the mean of $Y$ and the mean of $\bar{Y}$?
(b) the standard deviation of $Y$ and the standard deviation of $\bar{Y}$?(c) 
the standard deviation of $\bar{Y}$ and the standard error of $\bar{Y}$?(d) a 
t-ratio and a t-statistic?

(a) The mean of $Y$ is the population mean and as the mean of $\bar{Y}$ is the 
mean of the sampling distribution. In the limit as the sample size ($n$) goes to
infinite, the two values are equal.
(b) The standard deviation of the population ($Y$) is $\sigma$. The standard 
deviation of the sampling distribution is $\sigma/\sqrt{n}$. Notice that as 
$n \rightarrow \infty$ that the standard deviation of the sampling distribution
goes to zero, indicating that the population mean and the mean of the sampling
distribution converge.
(c) The standard deviation of $\bar{Y}$ and the standard error of $\bar{Y}$ are
the same thing. The standard error is called that because it tells us our best
guess about $\bar{Y} - \mu$. In other words, it's our best guess about how
far from the true mean our sample mean is.
(d) The t-ratio is the ratio of the error of the sample mean from the population
mean to the standard error of the sampling distribution. The t-ratio relates the
possible values of the population mean to a t-distribution with the appropriate
degrees of freedom based on the sample size. The t-statistic, on the other hand, 
is the ratio of the difference of means of two samples 
(of different populations) to a hypothesised difference in their population means
(typically taken to be zero) to the pooled standard error of the two samples.

*Addendum* - The t-ratio is generally not know because we don't actually know 
the population parameter. The t-statistic is a trial value of the t-ratio where
we use a hypothsised value of the parameter which we are testing with the NHST
framework.

### Problem 10
> Consider blood pressure levels for populations of young women using birth
control pills and young women not using birth control pills. A comparison of 
these populations through an observational study might be consisten with the 
theory that the pill elevates blood pressure levels. What tool is appropriate 
for addressing whether there is a difference between these two population? What 
tool is appropriate for addressing the likely size of the difference?

*p*-values are one way to access how likely it is that we would observe a 
difference equal to or more extreme than our data assuming that the null 
hypothesis of no difference is true.

A confidence interval constructed using the NHST framework will tell us what
percentage of sample means will fall within the range given by the confidence
interval. It is important to note that it doesn't say anything about whether our
data falls within this range. It is very easy to misinterpret frequentist
confidence intervals.

### Problem 11
> The data in Display 2.14 are survival times (in days) of guinea pigs that were
randomly assigned either to a control group or ato a treatment group that 
received a dose of tubercle bacilli. (a) Why might the additive treatment effect
model be inappropriate for these data? (b) Why might the ideal normal model with
equal spread be an inadequate approximation?

(a) The spread of data in the two groups doesn't suggest that the treatment 
group has merely experienced a shift in outcomes consistent with 
$Y^* = Y + \delta$. The control group has a larger variance (and also looks 
bimodal), so does not look like this model applies to this data.

(b) The ideal normal model doesn't apply here because the variance in the two 
groups is clearly different. The control group even appears to be bimodal, and 
so is not normally distributed.

## Computational Exercises

### Problem 12 - Marijuana Use During Pregnancy
> For the virth weights of babies in two groups, one born of mother who used 
marijuana during pregnancy and the other borm of mothers who did not, the 
difference in smaple averages (nonuser mothers minus user mothers) was 280 
grams, and the standard eorror of the difference was 46.66 grams with 1095 
degrees of freedom. From this information, provide the following: (a) a 95% 
confidence interval for $\mu_2 - \mu_1$, (c) a 90% confidence interval for 
$\mu_2 - \mu_1$,, and (c) the two-sided *p*-value for a test of the hypothesis 
that $\mu_2 - \mu_1 = 0$.



The test statistic for this hypothesis test is
$$
t = \frac{(\bar{Y}_2 - \bar{Y}_1) - 0}{\text{SE(difference)}} = \frac{280 - 0}{46.66} = `r round(280/46.66, 4)`
$$

```{r}
t_12 <- 280/46.66
ci_95 <- round(c(qt(0.025, df=1095), qt(0.975, df=1095)), 4)
ci_90 <- round(c(qt(0.05, df=1095), qt(0.95, df=1095)), 4)
p_12 <- 2*pt(t_12, df=1095, lower.tail = FALSE)
```



The 95% confidence interval is $[`r ci_95[1]`, `r ci_95[2]`]$.

The 90% confidence interval is $[`r ci_90[1]`, `r ci_90[2]`]$.

Because there are so many degrees of freedom, these t-distributions very closely
approximate the normal distribution, so the fact that these confidence intervals
closely resemble that of the standard normal is not surprising.

The two-sided *p*-value for this test is $p \approx `r round(p_12,4)`$ which makes sense
given the large ratio of the difference of means to the standard deviation of 
the difference in means.

### Problem 13
> Reconsider the changes in blood pressures for mean placed on a fish oil diet and for men placed on a regular oil diet. Do the following steps to compare the treatments:


(a) Compute the averates and the sample standard deviations for each group separately.
```{r}
fish_oil <- c(8,12,10,14,2,0,0)
regular_oil <- c(-6,0,1,2,-3,-4,2)

fish_bar <- mean(fish_oil)
regular_bar <- mean(regular_oil)

fish_s <- sd(fish_oil)
regular_s <- sd(regular_oil)
```

The average reduction in diastolic blood pressure for:

Fish oil group: $`r fish_bar`$
Regular oil group: $`r regular_bar`$

Standard deviations for the groups:

Fish oil group: $`r fish_s`$
Regular oil group: $`r regular_s`$

(b) Compute the pooled estimate of standard deviation using the formula in 
section 2.3.2.

The pooled standard deviation is just the square root of weighted average of the
variances.

$$
s_p = \sqrt{\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}}
$$

```{r}
n <- 14
n_1 <- n_2 <- n/2
s_p <- sqrt(((n_1-1)*fish_s^2 + (n_2-1)*regular_s^2)/(n_1 + n_2 - 2))
```

The pooled standard deviation is $`r round(s_p, 4)`$.

(c) Compute $SE(\bar{Y_2} - \bar{Y_1})$ using the formula in section 2.3.2.

```{r}
SE <- s_p*sqrt(1/n_1 + 1/n_2)
```

(d) What are the degrees of freedom associated with the pooled estimate of standard deviation? What is the 97.5th percentile of the *t*-distribution with this many degrees of freedom?

There are 12 degrees of freedom associated with the pooled standard deviation. 
We lose 2 degrees of freedom in computing the individual group standard 
deviations.

The 97.5th percentile for a t-distribution with 12 degrees of freedom is
$`r round(qt(0.975, df=12), 4)`$.

(e) Construct a 95% confidence interval for $\mu_2 - \mu_1$ using the formula in section 2.3.3.

```{r}
t_97.5 <- qt(0.975, df=12)

lower <- round((fish_bar - regular_bar) - SE*t_97.5,4)
upper <- round((fish_bar - regular_bar) + SE*t_97.5,4)
```
The 95% confidence interval is $[`r lower`, `r upper`]$.

(f) Compute the *t*-statistic for testing equality as shown in Section 2.3.5.

```{r}
t <- (fish_bar - regular_bar)/SE
```

The t-statistic for this test is $`r round(t, 4)`$.

(g) Find the one-sided *p*-value by comparing the *t*-statistic in (f) to the percentiles of the appropriate *t*-distribution.

The one-sided *p*-value for this test is 
$p=`r pt(t, df=12, lower.tail=FALSE)`$. Here, $p < 0.05$ and so we will
reject the null hypothesis that the means are equal at the $\alpha = 0.05$ 
level.

### Problem 14
> **Fish Oil and Blood Pressure**. Find the 95% confidence interval and one-sided *p*-valueasked for in Exercise 13(e) and (g) but use a statistical compute package to do so.

We'll use the `t.test` function from base R. We assume homoskedasticity by specifying `var.equal=TRUE`. This function produces very similar results to the manual calculation in problem 13. The *t*-statistic is very similar as is the *p*-value.

```{r}
t.test(fish_oil, regular_oil, alternative = "greater", var.equal = TRUE)
```
The confidence interval is a bit weird here. Not sure why the upper bound is 
$\infty$. The CI in problem 13 makes more sense, but I'm not sure which lower 
bound is correct. I can't any issues with the manually computed one in problem 
13. The other outputs agree nicely, so not sure what happened here.

### Problem 15
> **Auto exhaust and lead concentration in blood.** Researchers took independent
random samples from two populations of policemen and measure the level of lead
concentration in their blood. The sample of 126 police offices subjected to
constant inhalation of automobile exhaust fumes in downtown Cairo had an average
blood level concentration of $29.2\mu g/dl$ and an SD of $7.5 \mu g/dl$: a 
control sample of 50 policemen from the Cairo suburb of Abbasia, with no history
of exposure, had an average blood level concentration of $18.2 \mu g/dl$ and an 
SD of $5.8\mu g/dl$. Is there convincing evidence of a difference in the 
population means?

Here are the pieces of information that we're given:

- Average blood concentration for Abbasia police officers: $Y_{ap} = 18.2 \mu g/dl$
- Average blood concentration for Cairo police officers: $Y_{cp} = 29.2 \mu g/dl$
- Sample size of Abbasia police officers: $n_{ap} = 50$
- Sample size of Cairo police officers: $n_{cp} = 126$
- Standard deviation of blood concentration for Abbasia police officers: $s_{ap} = 5.8 \mu g/dl$
- Standard deviation of blood concentration for Cairo police officers: $s_{cp} = 7.5 \mu g/dl$

The difference in sample means is $Y_{cp} - Y_{ap} = 29.2 \mu g/dl - 18.2 \mu g/dl = 11.0 \mu g/dl$.

```{r}
n_cp <- 126
n_ap <- 50
Y_cp <- 29.2
Y_ap <- 18.2
mean_diff <- Y_cp - Y_ap
s_cp <- 7.5
s_ap <- 5.8

s_p <- sqrt(((n_cp - 1)*s_cp^2 + (n_ap - 1)*s_ap^2)/(n_cp + n_ap - 2))
SE <- s_p*sqrt(1/n_cp + 1/n_ap)
```

The pooled standard deviation for the two samples is given by
$$
\begin{align}
s_p &= \sqrt{\frac{(n_{cp} - 1)s_{cp}^2 + (n_{ap} - 1)s_{ap}^2}{n_{cp} + n_{ap} - 2}}\\[0.25cm]
 &= \sqrt{\frac{(126 - 1)7.5^2 + (50 - 1)5.8^2}{126 + 50 - 2}}\\[0.25cm]\\[0.25cm]
 &= `r round(s_p, 4)`
\end{align}
$$
The standard error of the sampling distribution is given by
$$
\begin{align}
SE(\bar{Y}_{cp} - \bar{Y}_{ap}) &= s_p\sqrt{\frac{1}{n_{cp}} + \frac{1}{n_{ap}}}\\[0.25cm]
&= `r round(s_p, 4)`\sqrt{\frac{1}{`r n_cp`} + \frac{1}{`r n_ap`}}\\[0.25cm]
&= `r round(SE, 4)`
\end{align}
$$
We have a total of $`r n_cp + n_ap`$ degrees of freedom, but since we're estimated the population standard deviations from data, we lose two of them, so our critical region will be defined by a *t*-distribution with $`r n_cp + n_ap - 2`$ degrees of freedom.

```{r}
t_97.5 <- qt(0.975, df=(n_cp + n_ap - 2))
t <- mean_diff/SE
```

The test statistic is computed using
$$
t = \frac{Y_{cp} - Y_{ap}}{SE} = \frac{`r Y_cp` - `r Y_ap`}{`r round(SE, 4)`} = `r round(t, 4)`
$$

The 95% confidence interval for the mean difference is
$$
\begin{align}
(\bar{Y}_{cp} - \bar{Y}_{ap}) &\pm t_{df}(1-\alpha/2)SE(\bar{Y}_{cp} - \bar{Y}_{ap})\\[0.25cm]
11.0 &\pm `r round(t_97.5, 4)`(`r round(SE,4)`)
\end{align}
$$
Note that a mean difference of zero is far from this confidence interval, so we expect a small *p*-value.

```{r}
p <- pt(t, df=(n_cp + n_ap - 2), lower.tail = FALSE)
```

$p = `r p`$. This *p*-value is **tiny**, so there is convincing evidence that the population means are different.

### Problem 16
> **Motivation and Creativity**. Verify the statements made in the summary of 
statistical findings for the Motivation and Creativity Data by analysing the 
data on the computer.

```{r problem 16}
intrinsic <- c(12.0,12.0,12.9,13.6,16.6,17.2,17.5,18.2,19.1,19.3,19.8,20.3,
               20.5,20.6,21.3,21.6,22.1,22.2,22.6,23.1,24.0,24.3,26.7,29.7)

extrinsic <- c(5.0,5.4,6.1,10.9,11.8,12.0,12.3,14.8,15.0,16.8,17.2,17.2,17.4,
               17.5,18.5,18.7,18.7,19.2,19.5,20.7,21.2,22.1,24.0)

n_i <- length(intrinsic)
n_e <- length(extrinsic)

mean_i <- mean(intrinsic)
mean_e <- mean(extrinsic)

sd_i <- sd(intrinsic)
sd_e <- sd(extrinsic)
```

Statistics

$$
\begin{align}
n_i = `r n_i`\\[0.25cm]
n_e = `r n_e`\\[0.25cm]
\bar{X}_i = `r round(mean_i,2)`\\[0.25cm]
\bar{X}_e = `r round(mean_e,2)`\\[0.25cm]
s_i = `r round(sd_i,2)`\\[0.25cm]
s_e = `r round(sd_e,2)`\\[0.25cm]
\end{align}
$$
```{r}
deg_f <- (n_i + n_e - 2)
s_p <- sqrt(((n_i - 1)*sd_i^2 + (n_e - 1)*sd_e^2)/(n_i + n_e - 2))
SE <- s_p*sqrt(1/n_i + 1/n_e)

t <- (mean_i - mean_e)/SE
t_97.5 <- qt(0.975, df=deg_f)

# compute two-sided p-value
p <- 2*pt(t, df=deg_f, lower.tail = FALSE)

lower <- (mean_i - mean_e) - t_97.5*SE
upper <- (mean_i - mean_e) + t_97.5*SE
```

Manually conducting the hypothesis test, we get the following results:

- Degrees of freedom: $`r deg_f`
- Pooled standard deviation: $`r round(s_p, 4)`$
- Standard Error: $`r round(SE, 4)`$
- Test statistic: $`r round(t,4)`$
- Critical region: $[-`r round(t_97.5, 4)`, `r round(t_97.5, 4)`]$
- 95% confidence interval: $[`r round(lower, 4)`, `r round(upper, 4)`]$
- *p*-value: $`r round(p, 4)`$


Hypothesis test using base R `t.test`:

```{r}
t.test(intrinsic, extrinsic, alternative = "two.sided", var.equal = TRUE)
```

Both the manual and `t.test` hypothesis tests produce results that closely match
those of the study's findings reported in statistical summary.

### Problem 17
>**Sex Discrimination**. Verify the statements made in the summary of statistical 
findings for the Sex Discrimination Data by analysing the ata on the computer.

```{r problem 17}
males <- c(4620,5040,5100,5100,5220,5400,5400,5400,5400,5400,5700,6000,6000,
           6000,6000,6000,6000,6000,6000,6000,6000,6000,6000,6000,6300,6600,
           6600,6600,6840,6900,6900,8100)
females <- c(3900,4020,4290,4380,4380,4380,4380,4380,4440,4500,4500,4620,4800,
             4800,4800,4800,4800,4800,4800,4800,4800,4800,4980,5100,5100,5100,
             5100,5100,5100,5160,5220,5220,5280,5280,5280,5400,5400,5400,5400,
             5400,5400,5400,5400,5400,5400,5400,5400,5520,5520,5580,5640,5700,
             5700,5700,5700,5700,6000,6000,6120,6300,6300)

n_m <- length(males)
n_f <- length(females)

mean_m <- mean(males)
mean_f <- mean(females)

sd_m <- sd(males)
sd_f <- sd(females)

deg_f <- n_m + n_f - 2
```

Basic statistics:

- Number of males: $`r n_m`$
- Number of females: $`r n_f`$
- Mean of males: $`r mean_m`$
- Mean of females: $`r mean_f`$
- Standard deviation of males: $`r sd_m`$
- Standard deviation of females: $`r sd_f`$
- Degrees of freedom: $`r deg_f`$

```{r}
s_p <- sqrt(((n_m - 1)*sd_m^2 + (n_f - 1)*sd_f^2)/deg_f)
SE <- s_p*sqrt(1/n_m + 1/n_f)
t <- (mean_m - mean_f)/SE
t_97.5 <- qt(0.975, df=deg_f)
lower <- (mean_m - mean_f) - t_97.5*SE
upper <- (mean_m - mean_f) + t_97.5*SE
p <- pt(t, df=deg_f, lower.tail = FALSE)
```

Manually conducting the hypothesis test, we get the following results:

- Pooled standard deviation: $`r round(s_p, 4)`$
- Standard Error: $`r round(SE, 4)`$
- Test statistic: $`r round(t,4)`$
- Critical region: $[-`r round(t_97.5, 4)`, `r round(t_97.5, 4)`]$
- 95% confidence interval: $[`r round(lower, 4)`, `r round(upper, 4)`]$
- *p*-value: $`r round(p, 4)`$

Hypothesis test using base R `t.test`:
```{r}
t.test(males, females, alternative = "greater", var.equal = TRUE)
```
### Problem 18
> **Bumpus's Data**. Verify the computations in Display 2.9 and Display 2.10 by 
analysing Bumpus's data on the computer.


```{r}
stat_died <- df_sparrows |>
  filter(survived == "Perished") |>
  summarise(
    n_p = n(),
    mean_p = mean(humerus_length_in),
    sd_p = sd(humerus_length_in)
  )

n_p <- stat_died |> pull(n_p)
mean_p <- stat_died |> pull(mean_p)
sd_p <- stat_died |> pull(sd_p)

stats_lived <- df_sparrows |>
  filter(survived == "Survived") |>
  summarise(
    n_l = n(),
    mean_l = mean(humerus_length_in),
    sd_l = sd(humerus_length_in)
  )

n_l <- stats_lived |> pull(n_l)
mean_l <- stats_lived |> pull(mean_l)
sd_l <- stats_lived |> pull(sd_l)

deg_f <- n_p + n_l - 2
```

Basic statistics:

- Number of perished: $`r n_p`$
- Number of survived: $`r n_l`$
- Mean of humerus length perished: $`r mean_p`$
- Mean of humerus length survived: $`r mean_l`$
- Standard deviation of humerus length perished: $`r sd_p`$
- Standard deviation of humerus length survived: $`r sd_l`$
- Degrees of freedom: $`r deg_f`$

```{r}
s_p <- sqrt(((n_p - 1)*sd_p^2 + (n_l - 1)*sd_l^2)/deg_f)
SE <- s_p*sqrt(1/n_p + 1/n_l)
t <- (mean_l - mean_p)/SE
t_97.5 <- qt(0.975, df=deg_f)

lower <- (mean_l - mean_p) - t_97.5*SE
upper <- (mean_l - mean_p) + t_97.5*SE

p <- 2*pt(t, df=deg_f)
```

Manually conducting the hypothesis test, we get the following results:

- Pooled standard deviation: $`r round(s_p, 4)`$
- Standard Error: $`r round(SE, 4)`$
- Test statistic: $`r round(t,4)`$
- Critical region: $[-`r round(t_97.5, 4)`, `r round(t_97.5, 4)`]$
- 95% confidence interval: $[`r round(lower, 4)`, `r round(upper, 4)`]$
- *p*-value (two-sided): $`r round(p, 4)`$

Hypothesis test using base R `t.test`:

```{r}
dead <- df_sparrows |>
  filter(survived == "Perished") |> 
  select(humerus_length_in) |> 
  pull()

alive <- df_sparrows |>
  filter(survived == "Survived") |> 
  select(humerus_length_in) |> 
  pull()
t.test(alive, dead, alternative = "two.sided", var.equal = TRUE)
```
These results are similar to those reported in the statistical summary.

### Problem 19


### Problem 20




## Data Exercises